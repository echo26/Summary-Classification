{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "    border-style: groove;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "    border-style: groove;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Categorizing summaries into the Original Text using NLP\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Summary\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>There are 3 different articles with different topics. Students will have speech about the article. They have to summarize article. Our goal is matching students' speech to one of article.</p>\n",
    "    <p>First, we will make bag of noun phrase for the 3 articles together. For each noun phrase, they will have count of appearances on each article. Then, we will give prediction on each noun phrase using Naive Bayes and we will make vector about this predictions.</p>\n",
    "    <p>Then, in order to compare each summary with the vector, we will manipulate summaries. First, we will remove all stop words and Interjections from each summaries. Then, we will collect nouns and match them with words in vectors. In matching process, instead of full string matching, we will use substring matching because the summaries information are not written text but speaking text. In other words, we accept the fact that summarizing in speaking is not well organized as much as written summarizing.</p>\n",
    "    <p>We will make a decision of categorizing using cosine similarity. The more higher similarity, the closer to the article.</p>\n",
    "    <p>At the end, we can know what article each summary is trying to describe.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/edward/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tree import Tree\n",
    "punctuation = string.punctuation\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()          #instantiate class\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treeb_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "1. Predictions about Noun Phrses from Article\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.1 Article to list of sentences.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>article_to_list function will make the text file to list of sentences </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_list(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        content = unicode(content, 'utf8').splitlines()\n",
    "    for text in content:\n",
    "        k = sent_tokenize(text)\n",
    "        for sent in k:\n",
    "            #unicode(sent).encode('UTF8')\n",
    "            result.append(sent)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = article_to_list(\"input1.txt\")\n",
    "article2 = article_to_list(\"input2.txt\")\n",
    "article3 = article_to_list(\"input3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.2 Make pandas table of sentenes.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "article_table = pd.DataFrame(columns=['id', 'sentence', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id=0\n",
    "for sentence in article1:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 1]\n",
    "        table_id+=1\n",
    "for sentence in article2:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 2]\n",
    "        table_id+=1\n",
    "for sentence in article3:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 3]\n",
    "        table_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Perspectives on Agriculture</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Learning objectives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Understand the ideas of Lewis Henry Morgan and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Explain the challenge to these ideas posed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Describe alternate theories / explanations to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           sentence label\n",
       "0  0                        Perspectives on Agriculture     1\n",
       "1  1                                Learning objectives     1\n",
       "2  2  Understand the ideas of Lewis Henry Morgan and...     1\n",
       "3  3  Explain the challenge to these ideas posed in ...     1\n",
       "4  4  Describe alternate theories / explanations to ...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.3 Make a bag of noun phrase. \n",
    "    \n",
    "    Part A, Extract NP\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Noun Phrase Extraction\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We will use Regular expression pattern matching with mltk chunker</p>\n",
    "    <p>In this project, we will define NP as the below.</p>\n",
    "    <p>The basic NP form is (Adjective | Noun)* (Noun Preposition)? (Adjective | Noun)* Noun</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_chunker = nltk.RegexpParser(r'''\n",
    "    NP:\n",
    "    {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS><IN|TO><DT>?<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS|PRP>*<NN|NNS|NNP|NNPS|PRP>} \n",
    "    {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS>}\n",
    "    {<NN|NNS|NNP|NNPS>*}\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "NP examples\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence\n",
      "The concepts developed by Morgan and Childe remain essential to the study of prehistory.\n"
     ]
    }
   ],
   "source": [
    "sent = article_table.loc[21, 'sentence']\n",
    "print(\"Original Sentence\")\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how Regex chunked it\n",
      "(S\n",
      "  The/DT\n",
      "  (NP concepts/NNS)\n",
      "  developed/VBN\n",
      "  by/IN\n",
      "  (NP Morgan/NNP)\n",
      "  and/CC\n",
      "  (NP Childe/NNP)\n",
      "  remain/VBP\n",
      "  essential/JJ\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (NP study/NN of/IN prehistory/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "chunks = rel_chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "print(\"how Regex chunked it\")\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_to_string(chunks):\n",
    "    np_list = []\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree: continue\n",
    "        if t.label() == 'NP':\n",
    "            li = []\n",
    "            for word, tag in t:\n",
    "                li.append(word)\n",
    "        np_list.append(' '.join(li))\n",
    "    return np_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of NP from the example sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'concepts', u'Morgan', u'Childe', u'study of prehistory']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"list of NP from the example sentence\")\n",
    "chunk_to_string(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.3 Make a bag of noun phrase. \n",
    "    \n",
    "    Part B, Make bag of noun phrase\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Bag of Noun Phrase\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We will use python dictionary for the bag of np.</p>\n",
    "    <p>Key value will be the NP and the values are list of 3 int.</p>\n",
    "    <p>3 int list stands for how many the np apears on article1, article2 and article3.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_np(table, chunker):\n",
    "    all_np = {}\n",
    "    size = len(table)\n",
    "    for i in range(size):\n",
    "        sentence = table.loc[i, 'sentence']\n",
    "        chunks = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        templist = chunk_to_string(chunks)\n",
    "        label = table.iloc[i]['label']\n",
    "        for np in templist:\n",
    "            if np in all_np.keys():\n",
    "                all_np[np][label-1] +=1\n",
    "            else:\n",
    "                all_np[np] = [0,0,0]\n",
    "                all_np[np][label-1] +=1\n",
    "    return all_np        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_np = count_np(article_table,rel_chunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Reason as Essential': [0, 1, 0],\n",
       " u'concept': [1, 0, 0],\n",
       " u'covert agenda': [0, 1, 0],\n",
       " u'ethical lapse in business': [0, 1, 0],\n",
       " u'ordinary people': [0, 1, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first5 ={k: all_np[k] for k in all_np.keys()[:5]}\n",
    "first5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.4 Naive Bayes\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We will use Naive Bayes and make prediction about the np.</p>\n",
    "    <p>The predictions will be used for vector in matching with summaries.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 112, 109]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Basic probabilities and class counts\n",
    "article1_count, article2_count, article3_count = article_table.groupby('label').size()\n",
    "total_count = [article1_count, article2_count, article3_count]\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_np(sentence, bag, counts, chunker):\n",
    "    ans_list = []\n",
    "    chunks = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "    np_list = chunk_to_string(chunks)\n",
    "    ##odd patter and wrangling\n",
    "    for i in range(len(counts)):\n",
    "        case_i = counts[i] / sum (count for count in counts)\n",
    "        num = 1.0\n",
    "        for word in np_list:\n",
    "            if word not in bag:\n",
    "                num *= 1\n",
    "            else:\n",
    "                num *= bag[word][i] / counts[i]\n",
    "        ans_list.append(num * case_i)\n",
    "    return tuple(ans_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Accuracy Check\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 50\n",
      "did 50\n",
      "did 50\n",
      "did 50\n",
      "did 50\n",
      "did 50\n",
      "did 50\n",
      "0.963954925537\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "copytable = article_table\n",
    "predictions = []\n",
    "val = 0\n",
    "for i,row in copytable.iterrows():\n",
    "    if i%50 ==0:\n",
    "        print('did 50')\n",
    "    pair = naive_bayes_np(row['sentence'], all_np, total_count, rel_chunker)\n",
    "    predictions.append(pair.index(max(pair))+1)\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = article_table['label']\n",
    "zipped = zip(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_dictionary = {(3, 3):0, (3, 2):0, (3, 1):0, (2, 3):0, (2, 2):0, (2, 1):0, (1, 3):0, (1, 2):0, (1, 1):0}\n",
    "for el in zipped:\n",
    "    confusion_dictionary[el] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (confusion_dictionary[(1,1)] + confusion_dictionary[(2,2)] + confusion_dictionary[(3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912790697674418"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0*correct/len(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>Wow. We got 99% accuracy in NB. It is unreal number in the real world. The reason for the consequence is the small input, first. We only have 3 different articles and the articles have 110-130 sentences. The input scale is not enough to be equally distributted. Also, the other reason is the uniqueness of the Nout Phrase. Rather than extracting Nouns, we choose to get NP. So, unique NPs makes each NP be unique.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "2. Match with Summaries\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.1 Summary to list of sentences\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaries_to_list(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        content = unicode(content, 'utf8').splitlines()\n",
    "    for i in range(len(content)):\n",
    "        obj = content[i].lower()\n",
    "        sent_li = []\n",
    "        if len(obj) ==0: continue\n",
    "        else:\n",
    "            k = sent_tokenize(obj)\n",
    "            for sent in k:\n",
    "                sent_li.append(sent)\n",
    "            result.append(sent_li)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_summaries = summaries_to_list(\"all_summaries.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.2 Remove speech words using swords and text wrangling idea \n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>Since the summaries are speech, we need to remove stop words and Interjections to make it more like the written summaries and more syntactical.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>remove IN, Preposition or subordinating conjunction, from swords.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlist = [] ## inlist: IN elements that need to be removed in stop words\n",
    "for word in sorted(swords):\n",
    "    chunks = rel_chunker.parse(nltk.pos_tag(nltk.word_tokenize(word)))\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree:\n",
    "            if t[1] ==\"IN\" or t[1] ==\"CC\":\n",
    "                inlist.append(t[0])\n",
    "for word in inlist:\n",
    "    swords.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>add Interjections and meaningless words to stop words.</p>\n",
    "    <p>This process is not fancy in Machine Learning point. But, just do it fist.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##meaningless words\n",
    "myswords = [\"so\", \"ok\", \"um\", \"uh\", \"mmm\", \"mm\", \"ah\", \"yeah\", \"hmm\", \"hm\", \"ph\",\n",
    "            \"oh\", \"things\", \"thing\", \"something\", \"category\",\"people\", \n",
    "            \"example\", \"examples\", \"article\", \"place\", \"other\",\"others\", \n",
    "            \"someone\", \"this\", \"that\", \"chapter\", \"just\", \"it\", \"topic\",\n",
    "            \"topics\", \"chapter\", \"chapters\", \"category\", \"anything\",\n",
    "           \"nope\", \"kind\", \"type\", \"types\", \"term\", \"terms\", \"means\",\n",
    "           \"type\", \"types\", \"kind\", \"maybe\", \"theory\", \"way\", \"author\",\n",
    "           \"task\", \"want\", \"textbook\", \"aspect\", \"aspects\", \"part\", \"lot\",\n",
    "           \"say\", \"get\", \"got\", \"talk\", \"talks\", \"think\", \"thinks\", \"mention\",\n",
    "           \"find\", \"finds\", \"remember\", \"anyone\", \"end\", \"ends\", \"intent\", \"c\"]\n",
    "for word in myswords:\n",
    "    swords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsAnyPunc(st, punctuation):\n",
    "    ##help function to filter string containing punctuation\n",
    "    return 1 in [c in st for c in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    ans = [[],[]]\n",
    "    word_tokes = word_punct_tokenizer.tokenize(sentence.lower())\n",
    "    for unit in word_tokes:\n",
    "        ##print(unit)\n",
    "        if containsAnyPunc(unit, punctuation) or unit in swords:\n",
    "            ans[1].append(unit)\n",
    "        else:\n",
    "            ans[0].append(unit)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>example of the fisrt summary how we get meaningful and syntactical sentences.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok, so, this chapter focused on um science and pure science and applied science.\n",
      "focused on science and pure science and applied science.\n",
      "-----------------\n",
      "um, pure science is just finding out things for the greater knowledge and applied science is for a specific task in trying to figure something out.\n",
      "pure science finding out for greater knowledge and applied science for specific in trying figure out.\n",
      "-----------------\n",
      "um, the chapter also discussed um, the defintion of life.\n",
      "also discussed defintion of life.\n",
      "-----------------\n",
      "and the different aspects which fall into that category.\n",
      "and different fall into.\n",
      "-----------------\n",
      "so, things like, um, homeostasis and other characteristics that help define life in it's different categories.\n",
      "like homeostasis and characteristics help define life in different categories.\n",
      "-----------------\n",
      "um, article talked about dna and what else was the chapter about?\n",
      "talked about dna and else about.\n",
      "-----------------\n",
      "mmm, yeah.\n",
      ".\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for sent in list_of_summaries[0]:\n",
    "    print(sent)\n",
    "    print(' '.join(sentence_wrangler(sent, swords, punctuation)[0]) +'.')\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_li = []\n",
    "for summary in list_of_summaries:\n",
    "    li = []\n",
    "    for sent in summary:\n",
    "        li.append(' '.join(sentence_wrangler(sent, swords, punctuation)[0]) +'.')\n",
    "        \n",
    "    sum_li.append(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.3 Make vector for the articles and summaries\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>First, we will sort the bag of NP from the articles.</p>\n",
    "    <p>Then, this sorted bag of np will be the used as empty vector with all 0 values.</p>\n",
    "    <p>count_up function will make the </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Sort bag of NP and make the empty vector\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(all_np.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_vector = []\n",
    "for key, li in sorted_items:\n",
    "    standard_vector.append([key, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>Since the summaries are speech text, we will extract Noun instead of NP.</p>\n",
    "    <p>In grading, we will consider that speech is not as perfect as the written text.</p>\n",
    "    <p>So, in matching the aritlces and summary, we will match if a noun from sumamry is a substring of NP in articles.</p>\n",
    "    <p>sum_vector function will create a vector with the same key with the articles vector, but about summary's nouns.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_chunker = nltk.RegexpParser(r'''\n",
    "    NOUN:\n",
    "    {<NN|NNS|NNP|NNPS>}\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_to_string(chunks):\n",
    "    np_list = []\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree: continue\n",
    "        if t.label() == 'NOUN':\n",
    "            li = []\n",
    "            for word, tag in t:\n",
    "                li.append(word)\n",
    "        np_list.append(' '.join(li))\n",
    "    return np_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_in_li(li, noun):\n",
    "    result = []\n",
    "    b = False\n",
    "    for i in range(len(li)):\n",
    "        if noun in li[i]:\n",
    "            b = True\n",
    "            result .append(i)\n",
    "    return [b, result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vector(summary, basic_vec, chunker):\n",
    "    not_used = []\n",
    "    for sent in summary:\n",
    "        chunks = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "        ##print(chunks)\n",
    "        templist = noun_to_string(chunks)\n",
    "        for np in templist:\n",
    "            #print(np)\n",
    "            status, idxli = noun_in_li(basic_vec, np)\n",
    "            if status:\n",
    "                for idx in idxli:\n",
    "                    basic_vec[idx][1] +=1\n",
    "                    #print(\"************************\")\n",
    "                    #print(basic_vec[idx])\n",
    "            else:\n",
    "                not_used.append((np, 1))\n",
    "    #print(\"-----------------------------\")\n",
    "    return list(count for word, count in basic_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.4 Matching Summary and categorize it.\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>Comparing Summary vector and aritlce vectors using cosine_similarity, we will make a predictions. The higher percentages, the closer similarity</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    num, deno1, deno2 = 0, 0, 0\n",
    "    if len(v1) != len(v2):\n",
    "        print(\"vector size not matched\")\n",
    "    else:\n",
    "        for i in range(len(v1)):\n",
    "            num += v1[i]*v2[i]\n",
    "            deno1 += v1[i]*v1[i]\n",
    "            deno2 += v2[i]*v2[i]\n",
    "        deno = math.sqrt(deno1*deno2)\n",
    "    return num/deno if deno !=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_vec = [pair[1][0] for pair in sorted_items]\n",
    "article2_vec = [pair[1][1] for pair in sorted_items]\n",
    "article3_vec = [pair[1][2] for pair in sorted_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_index(std, vec1, vec2, vec3):\n",
    "    v1 = cosine_similarity(std, vec1)\n",
    "    v2 = cosine_similarity(std, vec2)\n",
    "    v3 = cosine_similarity(std, vec3)\n",
    "    value = max(v1, v2 ,v3)\n",
    "    #print(v1,v2,v3)\n",
    "    if value == v1:\n",
    "        return 0\n",
    "    elif value == v2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorize_li = []\n",
    "for summary in sum_li:\n",
    "    vec = copy.deepcopy(standard_vector)\n",
    "    v = sum_vector(summary, vec, match_chunker)\n",
    "    idx = get_match_index(v, article1_vec, article2_vec, article3_vec)\n",
    "    categorize_li.append(idx+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Look Details about the result\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article1's count: 36\n",
      "article2's count: 37\n",
      "article3's count: 41\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1cf43510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = \"article1\", \"article2\", \"article3\"\n",
    "counter = collections.Counter(categorize_li)\n",
    "values = counter.values()\n",
    "for i in range(3):\n",
    "    print(\"{}'s count: {}\".format(labels[i], values[i]))\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(values, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Wrap Up\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We can see how many summaries are allocated for each articles From pie chart.</p>\n",
    "    <p>It seems like evenly populated.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Things to supplement\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>I used (Adjective | Noun)* (Noun Preposition)? (Adjective | Noun)* Noun as the NP. However, It was not able to match all NP correctly. For example, words like 'development of human and agriculture' it cuts into 'development of human' and 'agricultue'. So, we need advanced regular expression for NP with bridging words.</p>\n",
    "    <p>Also, the regular expression was not able to extract NP like 'increasing population' where VBG acts as Adjective. Moreover, NLTK has limit on classifying words with various tense. This problem has to be solved with analyze the whole sentence and need higher level </p>\n",
    "    <p>In removing stop words and speech words, I manually appended words. But, to make realizable program, we need to figure out automatic ways to recognize it.</p>\n",
    "    <p>We didn't used Neural Network and Pytorch. If we can apply that technology into this project, it will bring more accurate result.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
