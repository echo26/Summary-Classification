{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "    border-style: groove;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "    border-style: groove;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Categorizing Summaries\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Summary\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>There are 3 different articles with different topics. Students will have speech about 4 different articles; one additional article is unknown to us.  They summarized one of article. Our goal is matching each speech to one of article that student intended to summarize.</p>\n",
    "    <p>First, we will make bag of noun phrase (BOW) for the 3 articles together and count the appearance of each noun pharase. We will specifiy which article it was appeared. </p>\n",
    "    <p>Then, in order to categorize each summary, we will manipulate summaries. First, we will remove all stop words and Interjections from each summaries. Then, we will collect nouns and match them with words in vectors. (In matching process, instead of full string matching, we will use substring matching because the summaries information are not written text but speaking text. In other words, we accept the fact that summarizing in speaking is not well organized as much as written summarizing.)</p>\n",
    "    <p>We will make a decision of categorizing using cosine similarity. The higher similarity, the closer to the article.</p>\n",
    "    <p>At the end, we can know what article each summary is trying to describe.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Import Libraries\n",
    "</h2>\n",
    "<h2>\n",
    "Set target summary\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>pick one summary from all_summaries file </p>\n",
    "    <p>remove IN elements in stop words</p>\n",
    "    <p>add necessary elements to stop words</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/edward/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tree import Tree\n",
    "punctuation = string.punctuation\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()          #instantiate class\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treeb_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "1. Predictions about Noun Phrses from Article\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.1 Article to list of sentences.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>article_to_list function will make the text file to list of sentences </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_list(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        content = content.splitlines()\n",
    "#         content = unicode(content, 'utf8').splitlines()\n",
    "    for text in content:\n",
    "        k = sent_tokenize(text)\n",
    "        for sent in k:\n",
    "            #unicode(sent).encode('UTF8')\n",
    "            result.append(sent)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = article_to_list(\"input1.txt\")\n",
    "article2 = article_to_list(\"input2.txt\")\n",
    "article3 = article_to_list(\"input3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.2 Make pandas table of sentenes.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "article_table = pd.DataFrame(columns=['id', 'sentence', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id=0\n",
    "for sentence in article1:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 1]\n",
    "        table_id+=1\n",
    "for sentence in article2:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 2]\n",
    "        table_id+=1\n",
    "for sentence in article3:\n",
    "    if len(sentence) >=3:\n",
    "        article_table.loc[table_id] = [table_id, sentence, 3]\n",
    "        table_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Perspectives on Agriculture</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Learning objectives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Understand the ideas of Lewis Henry Morgan and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Explain the challenge to these ideas posed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Describe alternate theories / explanations to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           sentence label\n",
       "0  0                        Perspectives on Agriculture     1\n",
       "1  1                                Learning objectives     1\n",
       "2  2  Understand the ideas of Lewis Henry Morgan and...     1\n",
       "3  3  Explain the challenge to these ideas posed in ...     1\n",
       "4  4  Describe alternate theories / explanations to ...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.3 Make a bag of noun phrase. (BOW)\n",
    "    \n",
    "    Part A, Extract NP\n",
    "</h2>\n",
    "<h2>\n",
    "Noun Phrase Extraction\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We will use Regular expression pattern matching with nltk chunker</p>\n",
    "    <p>In this project, we will define NP as the below.</p>\n",
    "    <p>The basic NP form is (Adjective | Noun)* (Noun Preposition)? (Adjective | Noun)* Noun</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_chunker = nltk.RegexpParser(r'''\n",
    "    NP:\n",
    "    {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS><IN|TO><DT>?<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS|PRP>*<NN|NNS|NNP|NNPS|PRP>} \n",
    "    {<JJ|JJR|JJS>*<NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS>}\n",
    "    {<NN|NNS|NNP|NNPS>*}\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "NP examples\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence\n",
      "The concepts developed by Morgan and Childe remain essential to the study of prehistory.\n"
     ]
    }
   ],
   "source": [
    "sent = article_table.loc[21, 'sentence']\n",
    "print(\"Original Sentence\")\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how Regex chunked it\n",
      "(S\n",
      "  The/DT\n",
      "  (NP concepts/NNS)\n",
      "  developed/VBN\n",
      "  by/IN\n",
      "  (NP Morgan/NNP)\n",
      "  and/CC\n",
      "  (NP Childe/NNP)\n",
      "  remain/VBP\n",
      "  essential/JJ\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (NP study/NN of/IN prehistory/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "chunks = rel_chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "print(\"how Regex chunked it\")\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_to_string(chunks):\n",
    "    np_list = []\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree: continue\n",
    "        if t.label() == 'NP':\n",
    "            li = []\n",
    "            for word, tag in t:\n",
    "                li.append(word)\n",
    "        np_list.append(' '.join(li))\n",
    "    return np_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of NP from the example sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['concepts', 'Morgan', 'Childe', 'study of prehistory']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"list of NP from the example sentence\")\n",
    "chunk_to_string(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "1.3 Make a bag of noun phrase. \n",
    "    \n",
    "    Part B, Make bag of noun phrase\n",
    "</h2>\n",
    "<h2>\n",
    "Bag of Noun Phrase\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>We will use python dictionary for the bag of np.</p>\n",
    "    <p>Key value will be the NP and the values are list of 3 int.</p>\n",
    "    <p>3 int list stands for how many the np apears on article1, article2 and article3.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_np(table, chunker):\n",
    "    all_np = {}\n",
    "    size = len(table)\n",
    "    for i in range(size):\n",
    "        sentence = table.loc[i, 'sentence']\n",
    "        chunks = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        templist = chunk_to_string(chunks)\n",
    "        label = table.iloc[i]['label']\n",
    "        for np in templist:\n",
    "            if np in all_np.keys():\n",
    "                all_np[np][label-1] +=1\n",
    "            else:\n",
    "                all_np[np] = [0,0,0]\n",
    "                all_np[np][label-1] +=1\n",
    "    return all_np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_np = count_np(article_table,rel_chunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "2. Match with Summaries\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summaries_to_list(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "        content = content.splitlines()\n",
    "#         print(content)\n",
    "    for i in range(len(content)):\n",
    "        obj = content[i].lower()\n",
    "        sent_li = []\n",
    "        if len(obj) ==0: continue\n",
    "        else:\n",
    "#             print(obj.split(\"\\t\"))\n",
    "            text, label = obj.split(\"\\t\")\n",
    "            \n",
    "            k = sent_tokenize(text)\n",
    "            for sent in k:\n",
    "#                 print(sent, label)\n",
    "                sent_li.append([sent, label])\n",
    "            result.append(sent_li)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_summaries = summaries_to_list(\"all_summaries_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ok, so, this chapter focused on um science and pure science and applied science.',\n",
       "   '3'],\n",
       "  ['um, pure science is just finding out things for the greater knowledge and applied science is for a specific task in trying to figure something out.',\n",
       "   '3'],\n",
       "  ['um, the chapter also discussed um, the defintion of life.', '3'],\n",
       "  ['and the different aspects which fall into that category.', '3'],\n",
       "  [\"so, things like, um, homeostasis and other characteristics that help define life in it's different categories.\",\n",
       "   '3'],\n",
       "  ['um, article talked about dna and what else was the chapter about?', '3'],\n",
       "  ['mmm, yeah.', '3']],\n",
       " [['so, this chapter focused on agriculture and different perspectives on it, including morgan, who believed in savagery lead to barbarism and um, then he believed that um people um - well, there is domestication of animals, that was one of his examples as well as pottery - the arrival of pottery.',\n",
       "   '0'],\n",
       "  ['um, and then gordon child believed in the neolithic revolution, which was um - it focused more on the um - i want to say the domestication of animals as well.',\n",
       "   '0'],\n",
       "  ['and then, there were other opposing views to how agriculture came about.',\n",
       "   '0'],\n",
       "  [\"um, such as um people's social interactions with each other and how um the surplus of food assisted in that.\",\n",
       "   '0'],\n",
       "  ['um, what else?', '0'],\n",
       "  ['um, the first two individuals saw this agri - the coming of agriculture as a departure of humans from nature.',\n",
       "   '0'],\n",
       "  ['um, there was also an individual who believed in symbiotic relationships, so um, the - for instance, domestication of animals is going - was um assisting both parties, as opposed to simply humans showing their dominance over other species.',\n",
       "   '0']]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_summaries[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.2 Remove speech words using swords and text wrangling idea \n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>Since the summaries are speech, we need to remove stop words and Interjections to make it more like the written summaries and more syntactical.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>remove IN, Preposition or subordinating conjunction, from swords.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlist = [] ## inlist: IN elements that need to be removed in stop words\n",
    "for word in sorted(swords):\n",
    "    chunks = rel_chunker.parse(nltk.pos_tag(nltk.word_tokenize(word)))\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree:\n",
    "            if t[1] ==\"IN\" or t[1] ==\"CC\":\n",
    "                inlist.append(t[0])\n",
    "for word in inlist:\n",
    "    swords.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>add Interjections and meaningless words to stop words.</p>\n",
    "    <p>This process is not fancy in Machine Learning point. But, just do it fist.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##meaningless words\n",
    "myswords = [\"so\", \"ok\", \"um\", \"uh\", \"mmm\", \"mm\", \"ah\", \"yeah\", \"hmm\", \"hm\", \"ph\",\n",
    "            \"oh\", \"things\", \"thing\", \"something\", \"category\",\"people\", \n",
    "            \"example\", \"examples\", \"article\", \"place\", \"other\",\"others\", \n",
    "            \"someone\", \"this\", \"that\", \"chapter\", \"just\", \"it\", \"topic\",\n",
    "            \"topics\", \"chapter\", \"chapters\", \"category\", \"anything\",\n",
    "           \"nope\", \"kind\", \"type\", \"types\", \"term\", \"terms\", \"means\",\n",
    "           \"type\", \"types\", \"kind\", \"maybe\", \"theory\", \"way\", \"author\",\n",
    "           \"task\", \"want\", \"textbook\", \"aspect\", \"aspects\", \"part\", \"lot\",\n",
    "           \"say\", \"get\", \"got\", \"talk\", \"talks\", \"think\", \"thinks\", \"mention\",\n",
    "           \"find\", \"finds\", \"remember\", \"anyone\", \"end\", \"ends\", \"intent\", \"c\"]\n",
    "for word in myswords:\n",
    "    swords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsAnyPunc(st, punctuation):\n",
    "    ##help function to filter string containing punctuation\n",
    "    return 1 in [c in st for c in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    ans = [[],[]]\n",
    "    word_tokes = word_punct_tokenizer.tokenize(sentence.lower())\n",
    "    for unit in word_tokes:\n",
    "        ##print(unit)\n",
    "        if containsAnyPunc(unit, punctuation) or unit in swords:\n",
    "            ans[1].append(unit)\n",
    "        else:\n",
    "            ans[0].append(unit)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>example of the fisrt summary how we get meaningful and syntactical sentences.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok, so, this chapter focused on um science and pure science and applied science.\n",
      "focused on science and pure science and applied science.\n",
      "-----------------\n",
      "um, pure science is just finding out things for the greater knowledge and applied science is for a specific task in trying to figure something out.\n",
      "pure science finding out for greater knowledge and applied science for specific in trying figure out.\n",
      "-----------------\n",
      "um, the chapter also discussed um, the defintion of life.\n",
      "also discussed defintion of life.\n",
      "-----------------\n",
      "and the different aspects which fall into that category.\n",
      "and different fall into.\n",
      "-----------------\n",
      "so, things like, um, homeostasis and other characteristics that help define life in it's different categories.\n",
      "like homeostasis and characteristics help define life in different categories.\n",
      "-----------------\n",
      "um, article talked about dna and what else was the chapter about?\n",
      "talked about dna and else about.\n",
      "-----------------\n",
      "mmm, yeah.\n",
      ".\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for sent, label in list_of_summaries[0]:\n",
    "    print(sent)\n",
    "    print(' '.join(sentence_wrangler(sent, swords, punctuation)[0]) +'.')\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_li = []\n",
    "for summary in list_of_summaries:\n",
    "    li = []\n",
    "    for sent, label in summary:\n",
    "        text = ' '.join(sentence_wrangler(sent, swords, punctuation)[0]) +'.'\n",
    "        if text != \".\":\n",
    "            li.append([text, label])\n",
    "        \n",
    "    sum_li.append(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.3 Make vector for the articles and summaries\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>First, we will sort the bag of NP from the articles.</p>\n",
    "    <p>Then, this sorted bag of np will be the used as empty vector with all 0 values.</p>\n",
    "    <p>count_up function will make the </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Sort bag of NP and make the empty vector\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(all_np.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_vector = []\n",
    "for key, li in sorted_items:\n",
    "    standard_vector.append([key, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "    <p>Since the summaries are speech text, we will extract Noun instead of NP.</p>\n",
    "    <p>In grading, we will consider that speech is not as perfect as the written text.</p>\n",
    "    <p>So, in matching the aritlces and summary, we will match if a noun from sumamry is a substring of NP in articles.</p>\n",
    "    <p>sum_vector function will create a vector with the same key with the articles vector, but about summary's nouns.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_chunker = nltk.RegexpParser(r'''\n",
    "    NOUN:\n",
    "    {<NN|NNS|NNP|NNPS>}\n",
    "   ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_to_string(chunks):\n",
    "    np_list = []\n",
    "    for t in chunks:\n",
    "        if type(t) != Tree: continue\n",
    "        if t.label() == 'NOUN':\n",
    "            li = []\n",
    "            for word, tag in t:\n",
    "                li.append(word)\n",
    "        np_list.append(' '.join(li))\n",
    "    return np_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_in_li(li, noun):\n",
    "    result = []\n",
    "    b = False\n",
    "    for i in range(len(li)):\n",
    "        if noun in li[i]:\n",
    "            b = True\n",
    "            result .append(i)\n",
    "    return [b, result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vector(summary, basic_vec, chunker):\n",
    "    not_used = []\n",
    "    sent, label = summary[0], summary[1]\n",
    "#     for sent, label in summary:\n",
    "    chunks = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    ##print(chunks)\n",
    "    templist = noun_to_string(chunks)\n",
    "    for np in templist:\n",
    "        #print(np)\n",
    "        status, idxli = noun_in_li(basic_vec, np)\n",
    "        if status:\n",
    "            for idx in idxli:\n",
    "                basic_vec[idx][1] +=1\n",
    "                #print(\"************************\")\n",
    "                #print(basic_vec[idx])\n",
    "        else:\n",
    "            not_used.append((np, 1))\n",
    "    #print(\"-----------------------------\")\n",
    "    return list(count for word, count in basic_vec), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "2.4 Matching Summary and categorize it using cosine similarity.\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "    <p>Comparing Summary vector and aritlce vectors using cosine_similarity, we will make a predictions. The higher percentages, the closer similarity</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    num, deno1, deno2 = 0, 0, 0\n",
    "    if len(v1) != len(v2):\n",
    "        print(\"vector size not matched\")\n",
    "        return -1\n",
    "    else:\n",
    "        for i in range(len(v1)):\n",
    "            num += v1[i]*v2[i]\n",
    "            deno1 += v1[i]*v1[i]\n",
    "            deno2 += v2[i]*v2[i]\n",
    "        deno = math.sqrt(deno1*deno2)\n",
    "    result = num/deno if deno !=0 else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1_vec = [pair[1][0] for pair in sorted_items]\n",
    "article2_vec = [pair[1][1] for pair in sorted_items]\n",
    "article3_vec = [pair[1][2] for pair in sorted_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_index(std, vec1, vec2, vec3):\n",
    "    v1 = cosine_similarity(std, vec1)\n",
    "    v2 = cosine_similarity(std, vec2)\n",
    "    v3 = cosine_similarity(std, vec3)\n",
    "    value = max(v1, v2 ,v3)\n",
    "    #print(v1,v2,v3)\n",
    "    if value == v1:\n",
    "        return 0\n",
    "    elif value == v2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totext(summary):\n",
    "    result = ''\n",
    "    l = 0\n",
    "    for sent, label in summary:\n",
    "        l = label\n",
    "        result += sent\n",
    "#     print(result, l)\n",
    "    return [result, l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Accuracy Check\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_li = []\n",
    "actual_li = []\n",
    "for summary in sum_li:\n",
    "    vec = copy.deepcopy(standard_vector)\n",
    "    summary = totext(summary)\n",
    "    v, label = sum_vector(summary, vec, match_chunker)\n",
    "    idx = get_match_index(v, article1_vec, article2_vec, article3_vec)\n",
    "    actual_li.append(int(label))\n",
    "    categorize_li.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(actual_li, categorize_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_dictionary = {(3, 3):0, (3, 2):0, (3, 1):0, (3, 0):0,\n",
    "                        (2, 3):0, (2, 2):0, (2, 1):0, (2, 0):0,\n",
    "                        (1, 3):0, (1, 2):0, (1, 1):0, (1, 0):0,\n",
    "                        (0, 3):0, (0, 2):0, (0, 1):0, (0, 0):0}\n",
    "length = 0\n",
    "for el in zipped:\n",
    "    if el[0] ==3:\n",
    "        pass\n",
    "    else:\n",
    "        length +=1\n",
    "        confusion_dictionary[el] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (confusion_dictionary[(0,0)] + confusion_dictionary[(1,1)] + confusion_dictionary[(2,2)] + confusion_dictionary[(3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529411764705882"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0*correct/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
